\relax 
\newlabel{FirstPage}{{}{1}{}{}{}}
\@writefile{toc}{\contentsline {title}{Nonparametric Clustering from Energy Statistics}{1}{}}
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{}}
\citation{Szkely2013}
\citation{RizzoVariance}
\citation{RizzoClustering}
\citation{Lyons}
\citation{Sejdinovic2013}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{2}{}}
\citation{Mercer}
\citation{Smola,Girolami}
\citation{Filippone}
\citation{Sejdinovic2013}
\citation{Dhillon2,Dhillon}
\citation{Szkely2013}
\citation{Sejdinovic2013}
\citation{Szkely2013}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background on Energy Statistics and RKHS}{4}{}}
\newlabel{sec:background}{{II}{4}{}{}{}}
\newlabel{eq:energy}{{1}{4}{}{}{}}
\newlabel{eq:energy2}{{2}{4}{}{}{}}
\citation{Sejdinovic2013}
\citation{Aronszajn}
\newlabel{eq:negative_type}{{3}{5}{}{}{}}
\newlabel{eq:energy3}{{4}{5}{}{}{}}
\citation{Gretton2012}
\citation{Berg1984}
\citation{Sejdinovic2013}
\citation{Sejdinovic2013}
\newlabel{eq:mmd}{{5}{6}{}{}{}}
\newlabel{eq:mmd2}{{6}{6}{}{}{}}
\newlabel{eq:inner_data}{{7}{6}{}{}{}}
\newlabel{eq:kernel_semimetric}{{8}{6}{}{}{}}
\newlabel{eq:gen_kernel}{{9}{6}{}{}{}}
\citation{Szkely2013}
\citation{Szkely2013}
\newlabel{eq:Erho}{{10}{7}{}{}{}}
\newlabel{eq:g_def}{{11}{7}{}{}{}}
\newlabel{eq:within}{{12}{7}{}{}{}}
\newlabel{eq:between}{{13}{7}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Clustering Based on Energy Statistics}{8}{}}
\newlabel{sec:clustering_theory}{{III}{8}{}{}{}}
\newlabel{th:minimize}{{1}{8}{}{}{}}
\newlabel{eq:minimize}{{14}{8}{}{}{}}
\newlabel{eq:kernel_matrix}{{16}{9}{}{}{}}
\newlabel{eq:label_matrix}{{17}{9}{}{}{}}
\newlabel{th:qcqp2}{{2}{9}{}{}{}}
\newlabel{eq:qcqp2}{{18}{9}{}{}{}}
\newlabel{eq:W2}{{19}{9}{}{}{}}
\newlabel{eq:max_prob}{{20}{9}{}{}{}}
\citation{NgJordan}
\newlabel{eq:qcqp}{{22}{10}{}{}{}}
\citation{Dhillon2,Dhillon}
\newlabel{eq:kernel_kmeans}{{24}{11}{}{}{}}
\newlabel{th:kernel_kmeans}{{3}{11}{}{}{}}
\newlabel{eq:J}{{25}{11}{}{}{}}
\citation{Dhillon}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Clustering Based on Weighted Energy Statistics}{12}{}}
\newlabel{eq:g_def2}{{27}{12}{}{}{}}
\newlabel{eq:minimize2}{{28}{12}{}{}{}}
\newlabel{eq:weighted_matrices}{{29}{13}{}{}{}}
\newlabel{th:qcqp3}{{4}{13}{}{}{}}
\newlabel{eq:qcqp3}{{30}{13}{}{}{}}
\citation{Dhillon}
\citation{Kernighan,Malik,Chan,Yu}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Connection with Graph Partioning}{14}{}}
\newlabel{eq:assoc}{{36}{14}{}{}{}}
\newlabel{eq:cut}{{37}{14}{}{}{}}
\newlabel{eq:metric_graphs}{{40}{15}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Two-Class Problem in One Dimension}{15}{}}
\newlabel{sec:twoclass}{{IV}{15}{}{}{}}
\newlabel{eq:g_ind}{{42}{15}{}{}{}}
\newlabel{eq:g1d}{{43}{15}{}{}{}}
\citation{Vassilvitskii}
\newlabel{algo1d}{{1}{16}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces   Approximate solution to \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 14{}{}{}\hbox {}\unskip \@@italiccorr )}} for a two-class problem in one dimension. \hspace  {\fill } }}{16}{}}
\newlabel{eq:w1d}{{44}{16}{}{}{}}
\newlabel{eq:accuracy}{{45}{16}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   Energy statistics clustering by Algorithm\nobreakspace  {}1{}{}{}\hbox {}, compared to $k$-means and GMM. Both clusters have the same number of points ($x$-axis), which are increased in each experiment. We sample $100$ times from the distributions in the histograms and the $y$-axis is the average value of the clustering accuracy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 45{}{}{}\hbox {}\unskip \@@italiccorr )}} (errorbars are standard error). (a) $x\sim \genfrac  {}{}{}1{1}{2}\left [ \mathcal  {N}(\mu _1,\sigma _1) + \mathcal  {N}(\mu _2,\sigma _2) \right ]$, $\mu _1 = 0$, $\mu _2 = 5$, $\sigma _1 = 1$, and $\sigma _2 = 2$. (b) $x\sim \genfrac  {}{}{}1{1}{2}\left [ e^{\mathcal  {N}(\mu _1,\sigma _1)} + e^{\mathcal  {N}(\mu _2,\sigma _2)} \right ]$, $\mu _1 = 0$, $\mu _2 = -1.5$, $\sigma _1 = 0.3$, and $\sigma _2 = 1.5$. }}{17}{}}
\newlabel{fig:1d}{{1}{17}{}{}{}}
\citation{Dhillon2,Dhillon}
\@writefile{toc}{\contentsline {section}{\numberline {V}Iterative Algorithm for Energy Statistics Clustering}{18}{}}
\newlabel{sec:algo}{{V}{18}{}{}{}}
\newlabel{eq:maxQ}{{46}{18}{}{}{}}
\newlabel{eq:costxij}{{47}{18}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Kernel $\bm  {k}$-Means Algorithm}{18}{}}
\newlabel{eq:Jell}{{48}{18}{}{}{}}
\newlabel{kmeans_algo}{{2}{19}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces  Kernel $k$-means algorithm to find an approximate solution to \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 18{}{}{}\hbox {}\unskip \@@italiccorr )}}. \hspace  {\fill } }}{19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Energy Cost Algorithm}{19}{}}
\newlabel{eq:changeQ}{{49}{20}{}{}{}}
\citation{Vassilvitskii}
\newlabel{algo}{{3}{21}{}{}{}}
\newlabel{stepmove}{{V\tmspace  +\thinmuskip {.1667em}B}{21}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces  Energy cost algorithm to find an approximate solution to \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 18{}{}{}\hbox {}\unskip \@@italiccorr )}}. \hspace  {\fill } }}{21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Numerical Experiments}{21}{}}
\newlabel{sec:numerics}{{VI}{21}{}{}{}}
\newlabel{eq:standard_metric}{{50}{21}{}{}{}}
\newlabel{eq:gauss1}{{51}{22}{}{}{}}
\newlabel{eq:gauss2}{{52}{22}{}{}{}}
\newlabel{eq:gauss3}{{53}{22}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   Effect of increasing the ambient dimension while keeping Bayes error fixed, for two clusters with normally distributed data with $100$ points in each cluster. (a) We increase $D$ as described in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 51{}{}{}\hbox {}\unskip \@@italiccorr )}}. The blue line correspond to Algorithm\nobreakspace  {}3{}{}{}\hbox {}, while the magenta line corresponds to kernel $k$-means, Algorithm\nobreakspace  {}2{}{}{}\hbox {}. (b) The same but with data following \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 52{}{}{}\hbox {}\unskip \@@italiccorr )}}. One notices that Algorithm\nobreakspace  {}3{}{}{}\hbox {} is more robust than the other ones. }}{23}{}}
\newlabel{fig:gauss}{{2}{23}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   Previous algorithms for unbalanced clusters, according to \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 53{}{}{}\hbox {}\unskip \@@italiccorr )}}.}}{23}{}}
\newlabel{fig:unbalanced}{{3}{23}{}{}{}}
\newlabel{eq:rhohalf}{{54}{23}{}{}{}}
\newlabel{eq:rhoe}{{55}{23}{}{}{}}
\citation{Dhillon}
\newlabel{eq:20gauss}{{56}{24}{}{}{}}
\newlabel{eq:20loggauss}{{57}{24}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Conclusion}{24}{}}
\newlabel{sec:conclusion}{{VII}{24}{}{}{}}
\bibdata{energy_clusteringNotes,biblio.bib}
\bibcite{Szkely2013}{{1}{}{{}}{{}}}
\bibcite{RizzoVariance}{{2}{}{{}}{{}}}
\bibcite{RizzoClustering}{{3}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   (a) Data normally distributed as in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 56{}{}{}\hbox {}\unskip \@@italiccorr )}}. We increase the number of points in each cluster to illustrate the statistical consistency of the algorithms. (b) The same experiment but for data following \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 57{}{}{}\hbox {}\unskip \@@italiccorr )}}. In both experiments, for each case we run every algorithm $100$ times and show the average results. One can see the better performance of energy statistics clustering, Algorithm\nobreakspace  {}3{}{}{}\hbox {}, and in particular by using the semimetric \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 55{}{}{}\hbox {}\unskip \@@italiccorr )}}. These two figures illustrate that energy statistics clustering is nonparametric since it works well for very different distributions. }}{25}{}}
\newlabel{fig:consist}{{4}{25}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{25}{}}
\bibcite{Lyons}{{4}{}{{}}{{}}}
\bibcite{Sejdinovic2013}{{5}{}{{}}{{}}}
\bibcite{Mercer}{{6}{}{{}}{{}}}
\bibcite{Smola}{{7}{}{{}}{{}}}
\bibcite{Girolami}{{8}{}{{}}{{}}}
\bibcite{Filippone}{{9}{}{{}}{{}}}
\bibcite{Dhillon2}{{10}{}{{}}{{}}}
\bibcite{Dhillon}{{11}{}{{}}{{}}}
\bibcite{Aronszajn}{{12}{}{{}}{{}}}
\bibcite{Gretton2012}{{13}{}{{}}{{}}}
\bibcite{Berg1984}{{14}{}{{}}{{}}}
\bibcite{NgJordan}{{15}{}{{}}{{}}}
\bibcite{Kernighan}{{16}{}{{}}{{}}}
\bibcite{Malik}{{17}{}{{}}{{}}}
\bibcite{Chan}{{18}{}{{}}{{}}}
\bibcite{Yu}{{19}{}{{}}{{}}}
\bibcite{Vassilvitskii}{{20}{}{{}}{{}}}
\bibstyle{unsrt}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\newlabel{LastBibItem}{{20}{27}{}{}{}}
\newlabel{LastPage}{{}{27}{}{}{}}
