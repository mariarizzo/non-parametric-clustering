\relax 
\newlabel{FirstPage}{{}{1}{}{}{}}
\@writefile{toc}{\contentsline {title}{Nonparametric Clustering from Energy Statistics}{1}{}}
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{}}
\citation{Szkely2013}
\citation{RizzoVariance}
\citation{RizzoClustering}
\citation{Szkely2013}
\citation{Kgroups}
\citation{Lyons}
\citation{Sejdinovic2013}
\citation{Lloyd,MacQueen,Forgy}
\citation{Lloyd}
\citation{Smola,Girolami}
\citation{Mercer}
\citation{Girolami}
\citation{Dhillon2,Dhillon}
\citation{Lloyd}
\citation{Filippone}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{2}{}}
\citation{Kgroups}
\citation{Dhillon,Dhillon2}
\citation{Hartigan}
\citation{Kgroups}
\citation{Telgarsky,Slonin}
\citation{Szkely2013}
\citation{Sejdinovic2013}
\citation{Szkely2013}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background on Energy Statistics and RKHS}{4}{}}
\newlabel{sec:background}{{II}{4}{}{}{}}
\newlabel{eq:energy}{{1}{4}{}{}{}}
\newlabel{eq:energy2}{{2}{4}{}{}{}}
\newlabel{eq:negative_type}{{3}{4}{}{}{}}
\citation{Sejdinovic2013}
\citation{Aronszajn}
\newlabel{eq:energy3}{{4}{5}{}{}{}}
\citation{Gretton2012}
\citation{Berg1984}
\citation{Sejdinovic2013}
\citation{Sejdinovic2013}
\citation{Szkely2013}
\newlabel{eq:mmd}{{5}{6}{}{}{}}
\newlabel{eq:mmd2}{{6}{6}{}{}{}}
\newlabel{eq:inner_data}{{7}{6}{}{}{}}
\newlabel{eq:kernel_semimetric}{{8}{6}{}{}{}}
\newlabel{eq:gen_kernel}{{9}{6}{}{}{}}
\newlabel{eq:Erho}{{10}{6}{}{}{}}
\citation{Szkely2013}
\newlabel{eq:g_def}{{11}{7}{}{}{}}
\newlabel{eq:within}{{12}{7}{}{}{}}
\newlabel{eq:between}{{13}{7}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Clustering Based on Energy Statistics}{7}{}}
\newlabel{sec:clustering_theory}{{III}{7}{}{}{}}
\citation{Kgroups}
\newlabel{th:minimize}{{1}{8}{}{}{}}
\newlabel{eq:minimize}{{14}{8}{}{}{}}
\newlabel{eq:kernel_matrix}{{16}{8}{}{}{}}
\newlabel{eq:label_matrix}{{17}{9}{}{}{}}
\newlabel{th:qcqp2}{{2}{9}{}{}{}}
\newlabel{eq:qcqp2}{{18}{9}{}{}{}}
\newlabel{eq:W2}{{19}{9}{}{}{}}
\newlabel{eq:max_prob}{{20}{9}{}{}{}}
\newlabel{eq:qcqp}{{22}{9}{}{}{}}
\citation{NgJordan}
\citation{Dhillon2,Dhillon}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Relation to Kernel $\bm  {k}$-Means}{11}{}}
\newlabel{eq:kernel_kmeans}{{24}{11}{}{}{}}
\newlabel{th:kernel_kmeans}{{3}{11}{}{}{}}
\newlabel{eq:J}{{25}{11}{}{}{}}
\citation{Dhillon2,Dhillon}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Clustering Based on Weighted Energy Statistics}{12}{}}
\newlabel{sec:weighted}{{IV}{12}{}{}{}}
\newlabel{eq:g_def2}{{27}{12}{}{}{}}
\newlabel{eq:minimize2}{{28}{12}{}{}{}}
\newlabel{eq:weighted_matrices}{{29}{12}{}{}{}}
\citation{Dhillon2,Dhillon}
\newlabel{th:qcqp3}{{4}{13}{}{}{}}
\newlabel{eq:qcqp3}{{30}{13}{}{}{}}
\citation{Kernighan,Malik,Chan,Yu}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Connection with Graph Partitioning}{14}{}}
\newlabel{eq:assoc}{{36}{14}{}{}{}}
\newlabel{eq:cut}{{37}{14}{}{}{}}
\newlabel{eq:metric_graphs}{{40}{15}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Two-Class Problem in One Dimension}{15}{}}
\newlabel{sec:twoclass}{{V}{15}{}{}{}}
\newlabel{eq:g_ind}{{42}{15}{}{}{}}
\newlabel{eq:g1d}{{43}{15}{}{}{}}
\newlabel{eq:w1d}{{44}{15}{}{}{}}
\citation{Vassilvitskii}
\newlabel{algo1d}{{1}{16}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces   Approximate solution to \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 14{}{}{}\hbox {}\unskip \@@italiccorr )}} for a two-class problem in one dimension. \hspace  {\fill } }}{16}{}}
\newlabel{eq:accuracy}{{45}{16}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   Energy statistics clustering by Algorithm\nobreakspace  {}1{}{}{}\hbox {} compared to $k$-means and GMM/EM. We have the same number of points in both clusters, and for each case we sample $100$ times from the distributions shown in the histograms. We plot the average value of \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 45{}{}{}\hbox {}\unskip \@@italiccorr )}} versus the total number of points (error bars are standard error). The dashed line indicates the best possible classification accuracy computed from Bayes error. (a) Data coming from \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 46{}{}{}\hbox {}\unskip \@@italiccorr )}}, where the optimal accuracy is $\approx 0.956$. (b) Data from \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 47{}{}{}\hbox {}\unskip \@@italiccorr )}}, where the optimal accuracy is $\approx 0.852$. }}{17}{}}
\newlabel{fig:1d}{{1}{17}{}{}{}}
\newlabel{eq:two_normal}{{46}{17}{}{}{}}
\newlabel{eq:two_lognormal}{{47}{17}{}{}{}}
\citation{Dhillon2,Dhillon}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Iterative Algorithms for Energy Statistics Clustering}{18}{}}
\newlabel{sec:algo}{{VI}{18}{}{}{}}
\newlabel{eq:maxQ}{{48}{18}{}{}{}}
\newlabel{eq:costxij}{{49}{18}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Kernel $\bm  {k}$-Means Algorithm}{18}{}}
\newlabel{eq:Jell}{{50}{18}{}{}{}}
\citation{Hartigan}
\newlabel{kmeans_algo}{{2}{19}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces  Kernel $k$-means algorithm to find a local solution to \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 18{}{}{}\hbox {}\unskip \@@italiccorr )}}. \hspace  {\fill } }}{19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Hartigan's Method for Energy Statistics Clustering}{19}{}}
\citation{Telgarsky}
\newlabel{eq:changeQ}{{51}{20}{}{}{}}
\newlabel{noempty}{{1}{20}{}{}{}}
\newlabel{diffmean}{{2}{20}{}{}{}}
\citation{Telgarsky}
\citation{Telgarsky}
\citation{Telgarsky}
\newlabel{algo}{{3}{21}{}{}{}}
\newlabel{stepmove}{{VI}{21}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces  Hartigan's method to find a local solution to \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 18{}{}{}\hbox {}\unskip \@@italiccorr )}}. \hspace  {\fill } }}{21}{}}
\citation{Slonin}
\citation{Slonin}
\citation{Vassilvitskii}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Numerical Experiments}{22}{}}
\newlabel{sec:numerics}{{VII}{22}{}{}{}}
\newlabel{eq:standard_metric}{{52}{22}{}{}{}}
\citation{Telgarsky}
\newlabel{eq:gauss1}{{53}{23}{}{}{}}
\newlabel{eq:gauss2}{{54}{23}{}{}{}}
\newlabel{eq:cov}{{55}{23}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   Comparison of Algorithm\nobreakspace  {}3{}{}{}\hbox {}, Algorithm\nobreakspace  {}2{}{}{}\hbox {}, standard $k$-means and GMM/EM as the number of dimensions increase in gaussian settings. We compute the average of \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 45{}{}{}\hbox {}\unskip \@@italiccorr )}} over $100$ samples with error bars being standard error. We have two clusters with $100$ points each. (a) Data as in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 53{}{}{}\hbox {}\unskip \@@italiccorr )}}, where the optimal accuracy from Bayes error is the dashed line equal to $\approx 0.86$. (b) Data from \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 54{}{}{}\hbox {}\unskip \@@italiccorr )}} with $q=1/2$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 55{}{}{}\hbox {}\unskip \@@italiccorr )}}. (c) Data from \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 54{}{}{}\hbox {}\unskip \@@italiccorr )}} with $q=1$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 55{}{}{}\hbox {}\unskip \@@italiccorr )}}. (d) Data from \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 54{}{}{}\hbox {}\unskip \@@italiccorr )}} with $q=2$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 55{}{}{}\hbox {}\unskip \@@italiccorr )}}. The optimal accuracy from Bayes error in (b--d) is $\approx 1$. }}{24}{}}
\newlabel{fig:gauss}{{2}{24}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   Comparison of Algorithm\nobreakspace  {}3{}{}{}\hbox {}, Algorithm\nobreakspace  {}2{}{}{}\hbox {}, $k$-means, and GMM/EM. The data is distributed as \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 56{}{}{}\hbox {}\unskip \@@italiccorr )}} where we make the clusters progressively more unbalanced. }}{25}{}}
\newlabel{fig:unbalanced}{{3}{25}{}{}{}}
\newlabel{eq:gauss3}{{56}{25}{}{}{}}
\newlabel{eq:rhohalf}{{57}{25}{}{}{}}
\newlabel{eq:rhoe}{{58}{25}{}{}{}}
\newlabel{eq:20gauss}{{59}{25}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   Algorithm\nobreakspace  {}3{}{}{}\hbox {} and Algorithm\nobreakspace  {}2{}{}{}\hbox {} with kernels \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 52{}{}{}\hbox {}\unskip \@@italiccorr )}}, \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 57{}{}{}\hbox {}\unskip \@@italiccorr )}} and \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 58{}{}{}\hbox {}\unskip \@@italiccorr )}}, $k$-means, and GMM. The optimal accuracy in both cases is $\approx 0.9$. We show the average of \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 45{}{}{}\hbox {}\unskip \@@italiccorr )}} over $100$ samples with standard error. (a) Data distributed as in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 59{}{}{}\hbox {}\unskip \@@italiccorr )}}. (b) Data distributed as in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 60{}{}{}\hbox {}\unskip \@@italiccorr )}}. }}{26}{}}
\newlabel{fig:consist}{{4}{26}{}{}{}}
\newlabel{eq:20loggauss}{{60}{26}{}{}{}}
\newlabel{eq:rhoalpha}{{61}{26}{}{}{}}
\newlabel{eq:rhoesigma}{{62}{26}{}{}{}}
\newlabel{eq:rhogsigma}{{63}{26}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  (a) Parallel cigars. (b) Two concentric circles with noise. (c) Three concentric circles with noise. (d) MNIST handwritten digits. Clustering results are in Table\nobreakspace  {}I{}{}{}\hbox {} and Table\nobreakspace  {}II{}{}{}\hbox {}. }}{27}{}}
\newlabel{fig:other}{{5}{27}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces  Clustering the data shown in Fig.\nobreakspace  {}5{}{}{}\hbox {} with Algorithm\nobreakspace  {}3{}{}{}\hbox {} and Algorithm\nobreakspace  {}2{}{}{}\hbox {}, besides $k$-means and GMM, with kernels \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 61{}{}{}\hbox {}\unskip \@@italiccorr )}}--\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 63{}{}{}\hbox {}\unskip \@@italiccorr )}}. We sample $10$ times and show the average accuracy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 45{}{}{}\hbox {}\unskip \@@italiccorr )}} with standard error. }}{27}{}}
\newlabel{table:other}{{I}{27}{}{}{}}
\citation{Sapiro}
\newlabel{eq:sigma}{{64}{28}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces  Clustering the data shown in Fig.\nobreakspace  {}5{}{}{}\hbox {}d with Algorithm\nobreakspace  {}3{}{}{}\hbox {}, Algorithm\nobreakspace  {}2{}{}{}\hbox {}, and $k$-means. We use the kernel \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 61{}{}{}\hbox {}\unskip \@@italiccorr )}} with $\alpha \in \{1,2\}$ and the gaussian kernel \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 63{}{}{}\hbox {}\unskip \@@italiccorr )}} with $\sigma $ given by \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 64{}{}{}\hbox {}\unskip \@@italiccorr )}}. For each subset of digits we sample $10$ times and show the average accuracy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 45{}{}{}\hbox {}\unskip \@@italiccorr )}} with standard error. We sample $100$ points for each class. }}{28}{}}
\newlabel{table:mnist}{{II}{28}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Conclusion}{28}{}}
\newlabel{sec:conclusion}{{VIII}{28}{}{}{}}
\citation{Mahoney}
\citation{Becker}
\bibdata{energy_clusteringNotes,biblio.bib}
\bibcite{Szkely2013}{{1}{}{{}}{{}}}
\bibcite{RizzoVariance}{{2}{}{{}}{{}}}
\bibcite{RizzoClustering}{{3}{}{{}}{{}}}
\bibcite{Kgroups}{{4}{}{{}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Acknowledgements}{29}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{29}{}}
\bibcite{Lyons}{{5}{}{{}}{{}}}
\bibcite{Sejdinovic2013}{{6}{}{{}}{{}}}
\bibcite{Lloyd}{{7}{}{{}}{{}}}
\bibcite{MacQueen}{{8}{}{{}}{{}}}
\bibcite{Forgy}{{9}{}{{}}{{}}}
\bibcite{Smola}{{10}{}{{}}{{}}}
\bibcite{Girolami}{{11}{}{{}}{{}}}
\bibcite{Mercer}{{12}{}{{}}{{}}}
\bibcite{Dhillon2}{{13}{}{{}}{{}}}
\bibcite{Dhillon}{{14}{}{{}}{{}}}
\bibcite{Filippone}{{15}{}{{}}{{}}}
\bibcite{Hartigan}{{16}{}{{}}{{}}}
\bibcite{Telgarsky}{{17}{}{{}}{{}}}
\bibcite{Slonin}{{18}{}{{}}{{}}}
\bibcite{Aronszajn}{{19}{}{{}}{{}}}
\bibcite{Gretton2012}{{20}{}{{}}{{}}}
\bibcite{Berg1984}{{21}{}{{}}{{}}}
\bibcite{NgJordan}{{22}{}{{}}{{}}}
\bibcite{Kernighan}{{23}{}{{}}{{}}}
\bibcite{Malik}{{24}{}{{}}{{}}}
\bibcite{Chan}{{25}{}{{}}{{}}}
\bibcite{Yu}{{26}{}{{}}{{}}}
\bibcite{Vassilvitskii}{{27}{}{{}}{{}}}
\bibcite{Sapiro}{{28}{}{{}}{{}}}
\bibcite{Mahoney}{{29}{}{{}}{{}}}
\bibcite{Becker}{{30}{}{{}}{{}}}
\bibstyle{unsrt}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\newlabel{LastBibItem}{{30}{32}{}{}{}}
\newlabel{LastPage}{{}{32}{}{}{}}
