\relax 
\newlabel{FirstPage}{{}{1}{}{}{}}
\@writefile{toc}{\contentsline {title}{Energy Clustering}{1}{}}
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{}}
\citation{Szkely2013}
\citation{RizzoVariance}
\citation{RizzoClustering}
\citation{Kgroups}
\citation{Szkely2013}
\citation{Lyons}
\citation{Sejdinovic2013}
\citation{Lloyd,MacQueen,Forgy}
\citation{Lloyd}
\citation{Smola,Girolami}
\citation{Mercer}
\citation{Girolami}
\citation{Dhillon2,Dhillon}
\citation{Lloyd}
\citation{Filippone}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{2}{}}
\citation{Kgroups}
\citation{Dhillon,Dhillon2}
\citation{Hartigan}
\citation{Kgroups}
\citation{Telgarsky,Slonin}
\citation{Szkely2013}
\citation{Sejdinovic2013}
\citation{Szkely2013}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background on Energy Statistics and RKHS}{4}{}}
\newlabel{sec:background}{{II}{4}{}{}{}}
\newlabel{eq:energy}{{1}{4}{}{}{}}
\citation{Sejdinovic2013}
\newlabel{eq:energy2}{{2}{5}{}{}{}}
\newlabel{eq:negative_type}{{3}{5}{}{}{}}
\newlabel{eq:energy3}{{4}{5}{}{}{}}
\citation{Aronszajn}
\citation{Gretton2012}
\citation{Berg1984}
\newlabel{eq:mmd}{{5}{6}{}{}{}}
\newlabel{eq:mmd2}{{6}{6}{}{}{}}
\newlabel{eq:inner_data}{{7}{6}{}{}{}}
\newlabel{eq:kernel_semimetric}{{8}{6}{}{}{}}
\citation{Sejdinovic2013}
\citation{Sejdinovic2013}
\citation{Szkely2013}
\citation{Szkely2013}
\newlabel{eq:gen_kernel}{{9}{7}{}{}{}}
\newlabel{eq:Erho}{{10}{7}{}{}{}}
\newlabel{eq:g_def}{{11}{7}{}{}{}}
\newlabel{eq:within}{{12}{7}{}{}{}}
\newlabel{eq:between}{{13}{7}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Clustering Based on Energy Statistics}{8}{}}
\newlabel{sec:clustering_theory}{{III}{8}{}{}{}}
\newlabel{th:minimize}{{1}{8}{}{}{}}
\newlabel{eq:minimize}{{14}{8}{}{}{}}
\citation{Kgroups}
\newlabel{eq:kernel_matrix}{{16}{9}{}{}{}}
\newlabel{eq:label_matrix}{{17}{9}{}{}{}}
\newlabel{th:qcqp2}{{2}{9}{}{}{}}
\newlabel{eq:qcqp2}{{18}{9}{}{}{}}
\newlabel{eq:W2}{{19}{10}{}{}{}}
\newlabel{eq:max_prob}{{20}{10}{}{}{}}
\newlabel{eq:qcqp}{{22}{10}{}{}{}}
\newlabel{eq:relaxed}{{23}{10}{}{}{}}
\citation{NgJordan}
\citation{NgJordan}
\newlabel{spectralalgo}{{1}{11}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces   $\mathcal  {E}$-spectral is an exact method to solve the relaxed problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 23{}{}{}\hbox {}\unskip \@@italiccorr )}}. \hspace  {\fill } }}{11}{}}
\citation{Lloyd}
\citation{Dhillon2,Dhillon}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Relation to Kernel $\bm  {k}$-Means}{12}{}}
\newlabel{eq:kernel_kmeans}{{24}{12}{}{}{}}
\newlabel{th:kernel_kmeans}{{3}{12}{}{}{}}
\citation{Dhillon2,Dhillon}
\newlabel{eq:J}{{25}{13}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Clustering Based on Weighted Energy Statistics}{13}{}}
\newlabel{sec:weighted}{{IV}{13}{}{}{}}
\newlabel{eq:g_def2}{{27}{14}{}{}{}}
\newlabel{eq:minimize2}{{28}{14}{}{}{}}
\newlabel{eq:weighted_matrices}{{29}{14}{}{}{}}
\newlabel{th:qcqp3}{{4}{14}{}{}{}}
\newlabel{eq:qcqp3}{{30}{14}{}{}{}}
\citation{Dhillon2,Dhillon}
\citation{Kernighan,Malik,Chan,Yu}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Connection with Graph Partitioning}{15}{}}
\newlabel{eq:assoc}{{36}{15}{}{}{}}
\newlabel{eq:cut}{{37}{15}{}{}{}}
\newlabel{eq:metric_graphs}{{40}{16}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Two-Class Problem in One Dimension}{16}{}}
\newlabel{sec:twoclass}{{V}{16}{}{}{}}
\newlabel{eq:g_ind}{{42}{16}{}{}{}}
\newlabel{algo1d}{{2}{17}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces   $\mathcal  {E}^{1D}$-clustering algorithm to find local solutions to the optimization problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 14{}{}{}\hbox {}\unskip \@@italiccorr )}} for a two-class problem in one dimension. \hspace  {\fill } }}{17}{}}
\newlabel{eq:g1d}{{43}{17}{}{}{}}
\newlabel{eq:w1d}{{44}{17}{}{}{}}
\citation{Vassilvitskii}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   Energy statistics clustering by Algorithm\nobreakspace  {}2{}{}{}\hbox {} compared to $k$-means and GMM/EM. We have the same number of points in both clusters, and for each case we sample $100$ times from the distributions shown in the histograms. We plot the average value of \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 45{}{}{}\hbox {}\unskip \@@italiccorr )}} versus the total number of points (error bars are standard error). The dashed line indicates the best possible classification accuracy computed from Bayes error. (a) Data coming from \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 46{}{}{}\hbox {}\unskip \@@italiccorr )}}, where the optimal accuracy is $\approx 0.956$. (b) Data from \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 47{}{}{}\hbox {}\unskip \@@italiccorr )}}, where the optimal accuracy is $\approx 0.852$. }}{18}{}}
\newlabel{fig:1d}{{1}{18}{}{}{}}
\newlabel{eq:accuracy}{{45}{18}{}{}{}}
\citation{Dhillon2,Dhillon}
\newlabel{eq:two_normal}{{46}{19}{}{}{}}
\newlabel{eq:two_lognormal}{{47}{19}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Iterative Algorithms for Energy Clustering}{19}{}}
\newlabel{sec:algo}{{VI}{19}{}{}{}}
\newlabel{eq:maxQ}{{48}{19}{}{}{}}
\citation{Lloyd}
\citation{Dhillon2,Dhillon}
\newlabel{eq:costxij}{{49}{20}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Lloyd's Method for Energy Clustering}{20}{}}
\newlabel{eq:Jell}{{50}{20}{}{}{}}
\citation{Hartigan}
\newlabel{kmeans_algo}{{3}{21}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces  $\mathcal  {E}^{L}$-clustering: Lloyd's method for energy clustering, which is preciselly kernel $k$-means algorithm. This procedure finds local solutions to the problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 18{}{}{}\hbox {}\unskip \@@italiccorr )}}. }}{21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Hartigan's Method for Energy Clustering}{21}{}}
\newlabel{eq:changeQ}{{55}{22}{}{}{}}
\citation{Telgarsky}
\newlabel{algo}{{4}{23}{}{}{}}
\newlabel{stepmove}{{VI}{23}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces  $\mathcal  {E}^H$-clustering: Hartigan's method for energy clustering. This algorithm finds local solutions to the optimization problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 18{}{}{}\hbox {}\unskip \@@italiccorr )}}. \hspace  {\fill } }}{23}{}}
\newlabel{noempty}{{1}{23}{}{}{}}
\newlabel{diffmean}{{2}{23}{}{}{}}
\citation{Telgarsky}
\citation{Telgarsky}
\citation{Telgarsky}
\citation{Slonin}
\citation{Slonin}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Numerical Experiments}{24}{}}
\newlabel{sec:numerics}{{VII}{24}{}{}{}}
\citation{Vassilvitskii}
\citation{Telgarsky}
\newlabel{eq:standard_metric}{{57}{25}{}{}{}}
\newlabel{eq:gauss1}{{58}{25}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   Comparison of Algorithm\nobreakspace  {}4{}{}{}\hbox {}, Algorithm\nobreakspace  {}3{}{}{}\hbox {}, standard $k$-means and GMM/EM as the number of dimensions increase in Gaussian settings. We compute the average of \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 45{}{}{}\hbox {}\unskip \@@italiccorr )}} over $100$ samples with error bars being standard error. We have two clusters with $100$ points each. (a) Data as in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 58{}{}{}\hbox {}\unskip \@@italiccorr )}}, where the optimal accuracy from Bayes error is the dashed line equal to $\approx 0.86$. (b) Data from \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 59{}{}{}\hbox {}\unskip \@@italiccorr )}} with $q=1/2$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 60{}{}{}\hbox {}\unskip \@@italiccorr )}}. (c) Data from \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 59{}{}{}\hbox {}\unskip \@@italiccorr )}} with $q=1$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 60{}{}{}\hbox {}\unskip \@@italiccorr )}}. (d) Data from \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 59{}{}{}\hbox {}\unskip \@@italiccorr )}} with $q=2$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 60{}{}{}\hbox {}\unskip \@@italiccorr )}}. The optimal accuracy from Bayes error in (b--d) is $\approx 1$. }}{26}{}}
\newlabel{fig:gauss}{{2}{26}{}{}{}}
\newlabel{eq:gauss2}{{59}{27}{}{}{}}
\newlabel{eq:cov}{{60}{27}{}{}{}}
\newlabel{eq:gauss3}{{61}{27}{}{}{}}
\newlabel{eq:rhohalf}{{62}{27}{}{}{}}
\newlabel{eq:rhoe}{{63}{27}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   Comparison of Algorithm\nobreakspace  {}4{}{}{}\hbox {}, Algorithm\nobreakspace  {}3{}{}{}\hbox {}, $k$-means, and GMM/EM. The data is distributed as \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 61{}{}{}\hbox {}\unskip \@@italiccorr )}} where we make the clusters progressively more unbalanced. }}{28}{}}
\newlabel{fig:unbalanced}{{3}{28}{}{}{}}
\newlabel{eq:20gauss}{{64}{28}{}{}{}}
\newlabel{eq:20loggauss}{{65}{28}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   Algorithm\nobreakspace  {}4{}{}{}\hbox {} and Algorithm\nobreakspace  {}3{}{}{}\hbox {} with kernels \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 57{}{}{}\hbox {}\unskip \@@italiccorr )}}, \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 62{}{}{}\hbox {}\unskip \@@italiccorr )}} and \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 63{}{}{}\hbox {}\unskip \@@italiccorr )}}, $k$-means, and GMM. The optimal accuracy in both cases is $\approx 0.9$. We show the average of \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 45{}{}{}\hbox {}\unskip \@@italiccorr )}} over $100$ samples with standard error. (a) Data distributed as in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 64{}{}{}\hbox {}\unskip \@@italiccorr )}}. (b) Data distributed as in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 65{}{}{}\hbox {}\unskip \@@italiccorr )}}. }}{29}{}}
\newlabel{fig:consist}{{4}{29}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  (a) Parallel cigars. (b) Two concentric circles with noise. (c) Three concentric circles with noise. (d) MNIST handwritten digits. Clustering results are in Table\nobreakspace  {}I{}{}{}\hbox {} and Table\nobreakspace  {}II{}{}{}\hbox {}. }}{29}{}}
\newlabel{fig:other}{{5}{29}{}{}{}}
\newlabel{eq:rhoalpha}{{66}{29}{}{}{}}
\newlabel{eq:rhoesigma}{{67}{29}{}{}{}}
\newlabel{eq:rhogsigma}{{68}{29}{}{}{}}
\citation{Sapiro}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces  Clustering the data shown in Fig.\nobreakspace  {}5{}{}{}\hbox {} with Algorithm\nobreakspace  {}4{}{}{}\hbox {} and Algorithm\nobreakspace  {}3{}{}{}\hbox {}, with kernels \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 66{}{}{}\hbox {}\unskip \@@italiccorr )}}--\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 68{}{}{}\hbox {}\unskip \@@italiccorr )}}, as well as $k$-means and GMM. We sample $10$ times and show the average accuracy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 45{}{}{}\hbox {}\unskip \@@italiccorr )}} with standard error. }}{30}{}}
\newlabel{table:other}{{I}{30}{}{}{}}
\newlabel{eq:sigma}{{69}{30}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces  Clustering the data shown in Fig.\nobreakspace  {}5{}{}{}\hbox {}d with Algorithm\nobreakspace  {}4{}{}{}\hbox {}, Algorithm\nobreakspace  {}3{}{}{}\hbox {}, and $k$-means. We use the kernel \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 66{}{}{}\hbox {}\unskip \@@italiccorr )}} with $\alpha \in \{1,2\}$ and the gaussian kernel \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 68{}{}{}\hbox {}\unskip \@@italiccorr )}} with $\sigma $ given by \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 69{}{}{}\hbox {}\unskip \@@italiccorr )}}. For each subset of digits we sample $10$ times and show the average accuracy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 45{}{}{}\hbox {}\unskip \@@italiccorr )}} with standard error. We sample $100$ points for each class. }}{31}{}}
\newlabel{table:mnist}{{II}{31}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Discussion}{31}{}}
\newlabel{sec:conclusion}{{VIII}{31}{}{}{}}
\bibdata{energy_clusteringNotes,biblio.bib}
\bibcite{Szkely2013}{{1}{}{{}}{{}}}
\bibcite{RizzoVariance}{{2}{}{{}}{{}}}
\bibcite{RizzoClustering}{{3}{}{{}}{{}}}
\bibcite{Kgroups}{{4}{}{{}}{{}}}
\bibcite{Lyons}{{5}{}{{}}{{}}}
\bibcite{Sejdinovic2013}{{6}{}{{}}{{}}}
\bibcite{Lloyd}{{7}{}{{}}{{}}}
\bibcite{MacQueen}{{8}{}{{}}{{}}}
\bibcite{Forgy}{{9}{}{{}}{{}}}
\bibcite{Smola}{{10}{}{{}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Acknowledgements}{32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{32}{}}
\bibcite{Girolami}{{11}{}{{}}{{}}}
\bibcite{Mercer}{{12}{}{{}}{{}}}
\bibcite{Dhillon2}{{13}{}{{}}{{}}}
\bibcite{Dhillon}{{14}{}{{}}{{}}}
\bibcite{Filippone}{{15}{}{{}}{{}}}
\bibcite{Hartigan}{{16}{}{{}}{{}}}
\bibcite{Telgarsky}{{17}{}{{}}{{}}}
\bibcite{Slonin}{{18}{}{{}}{{}}}
\bibcite{Aronszajn}{{19}{}{{}}{{}}}
\bibcite{Gretton2012}{{20}{}{{}}{{}}}
\bibcite{Berg1984}{{21}{}{{}}{{}}}
\bibcite{NgJordan}{{22}{}{{}}{{}}}
\bibcite{Kernighan}{{23}{}{{}}{{}}}
\bibcite{Malik}{{24}{}{{}}{{}}}
\bibcite{Chan}{{25}{}{{}}{{}}}
\bibcite{Yu}{{26}{}{{}}{{}}}
\bibcite{Vassilvitskii}{{27}{}{{}}{{}}}
\bibcite{Sapiro}{{28}{}{{}}{{}}}
\bibstyle{unsrt}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\newlabel{LastBibItem}{{28}{34}{}{}{}}
\newlabel{LastPage}{{}{34}{}{}{}}
