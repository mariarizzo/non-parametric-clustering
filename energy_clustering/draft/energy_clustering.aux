\relax 
\newlabel{FirstPage}{{}{1}{}{}{}}
\@writefile{toc}{\contentsline {title}{Energy Clustering}{1}{}}
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{}}
\citation{Szkely2013}
\citation{RizzoVariance}
\citation{RizzoClustering}
\citation{Kgroups}
\citation{Szkely2013}
\citation{Lyons}
\citation{Sejdinovic2013}
\citation{Lloyd,MacQueen,Forgy}
\citation{Lloyd}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{2}{}}
\citation{Smola,Girolami}
\citation{Mercer}
\citation{Girolami}
\citation{Dhillon2,Dhillon}
\citation{Lloyd}
\citation{Filippone}
\citation{Kgroups}
\citation{Dhillon,Dhillon2}
\citation{Hartigan}
\citation{Kgroups}
\citation{Telgarsky,Slonin}
\citation{Szkely2013}
\citation{Sejdinovic2013}
\citation{Szkely2013}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background on Energy Statistics and RKHS}{4}{}}
\newlabel{sec:background}{{II}{4}{}{}{}}
\newlabel{eq:energy}{{1}{4}{}{}{}}
\newlabel{eq:energy2}{{2}{4}{}{}{}}
\citation{Sejdinovic2013}
\citation{Aronszajn}
\newlabel{eq:negative_type}{{3}{5}{}{}{}}
\newlabel{eq:energy3}{{4}{5}{}{}{}}
\citation{Gretton2012}
\citation{Berg1984}
\citation{Sejdinovic2013}
\citation{Sejdinovic2013}
\newlabel{eq:mmd}{{5}{6}{}{}{}}
\newlabel{eq:mmd2}{{6}{6}{}{}{}}
\newlabel{eq:inner_data}{{7}{6}{}{}{}}
\newlabel{eq:kernel_semimetric}{{8}{6}{}{}{}}
\newlabel{eq:gen_kernel}{{9}{6}{}{}{}}
\newlabel{eq:Erho}{{10}{6}{}{}{}}
\citation{Szkely2013}
\citation{Szkely2013}
\newlabel{eq:g_def}{{11}{7}{}{}{}}
\newlabel{eq:within}{{12}{7}{}{}{}}
\newlabel{eq:between}{{13}{7}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Clustering Based on Energy Statistics}{7}{}}
\newlabel{sec:clustering_theory}{{III}{7}{}{}{}}
\citation{Kgroups}
\newlabel{th:minimize}{{1}{8}{}{}{}}
\newlabel{eq:minimize}{{14}{8}{}{}{}}
\newlabel{eq:kernel_matrix}{{16}{9}{}{}{}}
\newlabel{eq:label_matrix}{{17}{9}{}{}{}}
\newlabel{th:qcqp2}{{2}{9}{}{}{}}
\newlabel{eq:qcqp2}{{18}{9}{}{}{}}
\newlabel{eq:W2}{{19}{9}{}{}{}}
\newlabel{eq:max_prob}{{20}{9}{}{}{}}
\citation{NgJordan}
\newlabel{eq:qcqp}{{22}{10}{}{}{}}
\newlabel{eq:relaxed}{{23}{10}{}{}{}}
\newlabel{spectralalgo}{{1}{11}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces   $\mathcal  {E^S}$-clustering is an exact spectral method to solve the relaxed problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 23{}{}{}\hbox {}\unskip \@@italiccorr )}}. \hspace  {\fill } }}{11}{}}
\citation{Lloyd}
\citation{Dhillon2,Dhillon}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Relation to Kernel $\bm  {k}$-Means}{12}{}}
\newlabel{eq:kernel_kmeans}{{24}{12}{}{}{}}
\newlabel{th:kernel_kmeans}{{3}{12}{}{}{}}
\newlabel{eq:J}{{25}{12}{}{}{}}
\citation{Dhillon2,Dhillon}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Clustering Based on Weighted Energy Statistics}{13}{}}
\newlabel{sec:weighted}{{IV}{13}{}{}{}}
\newlabel{eq:g_def2}{{27}{13}{}{}{}}
\newlabel{eq:minimize2}{{28}{13}{}{}{}}
\newlabel{eq:weighted_matrices}{{29}{14}{}{}{}}
\newlabel{th:qcqp3}{{4}{14}{}{}{}}
\newlabel{eq:qcqp3}{{30}{14}{}{}{}}
\citation{Dhillon2,Dhillon}
\citation{Kernighan,Malik,Chan,Yu}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Connection with Graph Partitioning}{15}{}}
\newlabel{eq:assoc}{{36}{15}{}{}{}}
\newlabel{eq:cut}{{37}{15}{}{}{}}
\newlabel{eq:metric_graphs}{{40}{16}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Two-Class Problem in One Dimension}{16}{}}
\newlabel{sec:twoclass}{{V}{16}{}{}{}}
\newlabel{eq:g_ind}{{42}{16}{}{}{}}
\newlabel{eq:g1d}{{43}{16}{}{}{}}
\citation{scikit-learn}
\citation{Vassilvitskii}
\newlabel{algo1d}{{2}{17}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces   $\mathcal  {E}^{1D}$-clustering algorithm to find local solutions to the optimization problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 14{}{}{}\hbox {}\unskip \@@italiccorr )}} for a two-class problem in one dimension. \hspace  {\fill } }}{17}{}}
\newlabel{eq:w1d}{{44}{17}{}{}{}}
\newlabel{eq:accuracy}{{45}{17}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   $\mathcal  {E}^{1D}$-clustering versus $k$-means and GMM in one dimension. The two clusters have the same number of points, and we use $50$ Monte Carlo runs. We plot the average accuracy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 45{}{}{}\hbox {}\unskip \@@italiccorr )}} versus the total number of points, where error bars are standard error. The dashed line indicates Bayes accuracy. (a) Data normally distributed as in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 46{}{}{}\hbox {}\unskip \@@italiccorr )}}, where the optimal accuracy is $\approx 0.956$. (b) Data log-normally distributed as in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 47{}{}{}\hbox {}\unskip \@@italiccorr )}} where the optimal accuracy is $\approx 0.852$. }}{18}{}}
\newlabel{fig:1d}{{1}{18}{}{}{}}
\newlabel{eq:two_normal}{{46}{18}{}{}{}}
\citation{Dhillon2,Dhillon}
\newlabel{eq:two_lognormal}{{47}{19}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Iterative Algorithms for Energy Clustering}{19}{}}
\newlabel{sec:algo}{{VI}{19}{}{}{}}
\newlabel{eq:maxQ}{{48}{19}{}{}{}}
\newlabel{eq:costxij}{{49}{19}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Lloyd's Method for Energy Clustering}{19}{}}
\newlabel{eq:Jell}{{50}{19}{}{}{}}
\citation{Lloyd}
\citation{Dhillon2,Dhillon}
\citation{Hartigan}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Hartigan's Method for Energy Clustering}{20}{}}
\newlabel{kmeans_algo}{{3}{21}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces  $\mathcal  {E}^{L}$-clustering is Lloyd's method for energy clustering, which is precisely\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {} \\ kernel $k$-means algorithm, with the kernel induced by energy statistics. This procedure\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {} \\ finds local solutions to the problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 18{}{}{}\hbox {}\unskip \@@italiccorr )}}.\hspace  {\fill } }}{21}{}}
\newlabel{eq:changeQ}{{55}{21}{}{}{}}
\newlabel{algo}{{4}{22}{}{}{}}
\newlabel{stepmove}{{VI}{22}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces  $\mathcal  {E}^H$-clustering is Hartigan's method for energy clustering. This algorithm\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {} \\ finds local solutions to the optimization problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 18{}{}{}\hbox {}\unskip \@@italiccorr )}}. The steps $6$ and $10$ are different\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {} \\ than $\mathcal  {E}^L$-clustering described in Algorithm\nobreakspace  {}3{}{}{}\hbox {}. \hspace  {\fill } }}{22}{}}
\citation{Telgarsky}
\citation{Telgarsky}
\citation{Telgarsky}
\citation{Telgarsky}
\newlabel{noempty}{{1}{23}{}{}{}}
\newlabel{diffmean}{{2}{23}{}{}{}}
\citation{Slonin}
\citation{Slonin}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Numerical Experiments}{24}{}}
\newlabel{sec:numerics}{{VII}{24}{}{}{}}
\newlabel{eq:standard_metric}{{57}{24}{}{}{}}
\citation{Telgarsky}
\citation{Vassilvitskii}
\citation{scikit-learn}
\newlabel{eq:gauss1}{{58}{25}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   Comparison of energy clustering algorithms to $k$-means and GMM in high dimensional Gaussian settings. We plot the mean accuracy versus the number of dimensions, with error bars indicating standard error from $100$ Monte Carlo runs. (a) Data distributed as in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 58{}{}{}\hbox {}\unskip \@@italiccorr )}}, with Bayes accuracy $\approx 0.86$, over the range $D \in [10,200]$. (b) Data distributed as in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 59{}{}{}\hbox {}\unskip \@@italiccorr )}}, with Bayes accuracy $\approx 0.95$, over the range $D \in [10, 700]$. }}{26}{}}
\newlabel{fig:gauss}{{2}{26}{}{}{}}
\newlabel{eq:gauss2}{{59}{26}{}{}{}}
\newlabel{eq:rhohalf}{{60}{27}{}{}{}}
\newlabel{eq:rhoe}{{61}{27}{}{}{}}
\newlabel{eq:20gauss}{{62}{27}{}{}{}}
\newlabel{eq:20loggauss}{{63}{27}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   $\mathcal  {E}^H$-clustering with different kernels versus $k$-means and GMM. In both settings Bayes accuracy is $\approx 0.9$. We show average accuracy (error bars are standard error) versus number of points for $100$ Monte Carlo trials. (a,b) Normal distribution \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 62{}{}{}\hbox {}\unskip \@@italiccorr )}}. (c,d) Lognormal distribution\nobreakspace  {}\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 63{}{}{}\hbox {}\unskip \@@italiccorr )}}. The plots in (c) and (d) consider the difference in accuracy between $\mathcal  {E}^H$ versus $\mathcal  {E}^L$ and $\mathcal  {E}^S$-clustering, with semimetric $\rho _e$. }}{28}{}}
\newlabel{fig:consist}{{3}{28}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  (a) Parallel cigars. (b) Two concentric circles with noise. (c) Three concentric circles with noise. (d) MNIST handwritten digits. Clustering results are in Table\nobreakspace  {}I{}{}{}\hbox {} and Table\nobreakspace  {}II{}{}{}\hbox {}. }}{28}{}}
\newlabel{fig:other}{{4}{28}{}{}{}}
\citation{Sapiro}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces  Clustering data from Fig.\nobreakspace  {}4{}{}{}\hbox {}. We show average accuracy over $10$ Monte Carlo runs. }}{29}{}}
\newlabel{table:other}{{I}{29}{}{}{}}
\newlabel{eq:rhoalpha}{{64}{29}{}{}{}}
\newlabel{eq:rhoesigma}{{65}{29}{}{}{}}
\newlabel{eq:rhogsigma}{{66}{29}{}{}{}}
\newlabel{eq:sigma}{{67}{29}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces  Clustering MNIST data from Fig.\nobreakspace  {}4{}{}{}\hbox {}d. For each subset of digits we perform $10$ Monte Carlo runs, sampling $100$ points for each class. }}{30}{}}
\newlabel{table:mnist}{{II}{30}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Discussion}{30}{}}
\newlabel{sec:conclusion}{{VIII}{30}{}{}{}}
\bibdata{energy_clusteringNotes,biblio.bib}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   Comparison of energy clustering algorithms to $k$-means and GMM on unbalanced clusters. The data is normally distributed as \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 68{}{}{}\hbox {}\unskip \@@italiccorr )}}, where we vary $m \in [0, 240]$, and in each case we do $100$ Monte Carlo runs showing the average accuracy with standard error. }}{31}{}}
\newlabel{fig:unbalanced}{{5}{31}{}{}{}}
\newlabel{eq:gauss3}{{68}{31}{}{}{}}
\bibcite{Szkely2013}{{1}{}{{}}{{}}}
\bibcite{RizzoVariance}{{2}{}{{}}{{}}}
\bibcite{RizzoClustering}{{3}{}{{}}{{}}}
\bibcite{Kgroups}{{4}{}{{}}{{}}}
\bibcite{Lyons}{{5}{}{{}}{{}}}
\bibcite{Sejdinovic2013}{{6}{}{{}}{{}}}
\bibcite{Lloyd}{{7}{}{{}}{{}}}
\bibcite{MacQueen}{{8}{}{{}}{{}}}
\bibcite{Forgy}{{9}{}{{}}{{}}}
\bibcite{Smola}{{10}{}{{}}{{}}}
\bibcite{Girolami}{{11}{}{{}}{{}}}
\bibcite{Mercer}{{12}{}{{}}{{}}}
\bibcite{Dhillon2}{{13}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {}Acknowledgments}{32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{32}{}}
\bibcite{Dhillon}{{14}{}{{}}{{}}}
\bibcite{Filippone}{{15}{}{{}}{{}}}
\bibcite{Hartigan}{{16}{}{{}}{{}}}
\bibcite{Telgarsky}{{17}{}{{}}{{}}}
\bibcite{Slonin}{{18}{}{{}}{{}}}
\bibcite{Aronszajn}{{19}{}{{}}{{}}}
\bibcite{Gretton2012}{{20}{}{{}}{{}}}
\bibcite{Berg1984}{{21}{}{{}}{{}}}
\bibcite{NgJordan}{{22}{}{{}}{{}}}
\bibcite{Kernighan}{{23}{}{{}}{{}}}
\bibcite{Malik}{{24}{}{{}}{{}}}
\bibcite{Chan}{{25}{}{{}}{{}}}
\bibcite{Yu}{{26}{}{{}}{{}}}
\bibcite{scikit-learn}{{27}{}{{}}{{}}}
\bibcite{Vassilvitskii}{{28}{}{{}}{{}}}
\bibcite{Sapiro}{{29}{}{{}}{{}}}
\bibstyle{unsrt}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\newlabel{LastBibItem}{{29}{34}{}{}{}}
\newlabel{LastPage}{{}{34}{}{}{}}
