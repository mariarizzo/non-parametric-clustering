\relax 
\citation{Szkely2013,Szkely2017}
\citation{RizzoVariance}
\citation{RizzoClustering}
\citation{Szkely2013,Szkely2017}
\citation{Kgroups}
\citation{Lyons}
\citation{Sejdinovic2013}
\citation{Lloyd,MacQueen,Forgy}
\citation{Lloyd}
\citation{Smola,Girolami}
\citation{Mercer}
\citation{Girolami}
\citation{Dhillon2,Dhillon}
\citation{Filippone}
\citation{Hartigan1975,Hartigan1979}
\citation{Telgarsky}
\citation{Telgarsky,Slonin}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{sec:introduction}{{1}{1}}
\citation{Kgroups}
\citation{Kgroups}
\citation{Szkely2013}
\citation{Lyons,Sejdinovic2013}
\citation{Szkely2013}
\citation{Sejdinovic2013}
\citation{Aronszajn}
\citation{Gretton2012}
\@writefile{toc}{\contentsline {section}{\numberline {2}Review of Energy Statistics and RKHS}{2}}
\newlabel{sec:background}{{2}{2}}
\newlabel{eq:energy}{{1}{2}}
\newlabel{eq:energy2}{{2}{2}}
\newlabel{eq:negative_type}{{3}{2}}
\newlabel{eq:energy3}{{4}{2}}
\newlabel{eq:mmd}{{5}{2}}
\citation{Berg1984}
\citation{Sejdinovic2013}
\citation{Sejdinovic2013}
\citation{Szkely2013}
\citation{Szkely2013}
\newlabel{eq:mmd2}{{6}{3}}
\newlabel{eq:kernel_semimetric}{{7}{3}}
\newlabel{eq:gen_kernel}{{8}{3}}
\newlabel{eq:Erho}{{9}{3}}
\newlabel{eq:g_def}{{10}{3}}
\newlabel{eq:within}{{11}{3}}
\newlabel{eq:between}{{12}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}The Clustering Problem Formulation}{3}}
\newlabel{sec:clustering_theory}{{3}{3}}
\newlabel{eq:g_def2}{{13}{3}}
\newlabel{eq:within2}{{15}{3}}
\newlabel{eq:between2}{{16}{3}}
\newlabel{th:minimize}{{1}{3}}
\newlabel{eq:minimize}{{17}{3}}
\citation{Malik,NgJordan}
\newlabel{eq:kernel_matrix}{{19}{4}}
\newlabel{eq:label_matrix}{{20}{4}}
\newlabel{eq:weighted_matrices}{{21}{4}}
\newlabel{th:qcqp3}{{2}{4}}
\newlabel{eq:qcqp3}{{22}{4}}
\newlabel{eq:W2}{{23}{4}}
\newlabel{eq:max_prob}{{24}{4}}
\newlabel{eq:relaxed}{{29}{4}}
\citation{Kernighan,Malik,Chan,Yu}
\citation{Dhillon2,Dhillon}
\citation{Dhillon2,Dhillon}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Connection with Graph Partitioning}{5}}
\newlabel{eq:assoc}{{31}{5}}
\newlabel{eq:cut}{{32}{5}}
\newlabel{eq:metric_graphs}{{35}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Connection with Kernel k-Means}{5}}
\newlabel{sec:kernel_kmeans}{{3.2}{5}}
\newlabel{eq:muj}{{37}{5}}
\newlabel{eq:kernel_kmeans}{{40}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Iterative Algorithms}{5}}
\newlabel{sec:algo}{{4}{5}}
\newlabel{eq:maxQ}{{41}{5}}
\citation{Dhillon2,Dhillon}
\citation{Lloyd}
\citation{Hartigan1975,Hartigan1979}
\citation{Kgroups}
\newlabel{eq:costxij}{{42}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Weighted Kernel k-Means Algorithm}{6}}
\newlabel{eq:min_lloyd}{{43}{6}}
\newlabel{eq:Jell}{{44}{6}}
\newlabel{eq:min_lloyd2}{{45}{6}}
\newlabel{kmeans_algo}{{1}{6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces  Weighted version of kernel k-means algorithm to find local solutions to the optimization problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 22\hbox {}\unskip \@@italiccorr )}}. }}{6}}
\newlabel{eq:closest_mean}{{46}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Kernel k-Groups Algorithm}{6}}
\newlabel{eq:deltaQ}{{47}{6}}
\newlabel{eq:Qplus}{{48}{6}}
\newlabel{eq:Qminus}{{49}{6}}
\newlabel{eq:changeQ}{{50}{6}}
\citation{scikit-learn}
\citation{Vassilvitskii}
\citation{Malik}
\newlabel{algo}{{2}{7}}
\newlabel{stepmove}{{4.2}{7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces  Kernel k-groups algorithm, based on Hartigan's method, to find local solutions to problem \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 22\hbox {}\unskip \@@italiccorr )}}. }}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Experiments}{7}}
\newlabel{sec:numerics}{{5}{7}}
\newlabel{eq:rho_alpha}{{52}{7}}
\newlabel{eq:rho_tilde}{{53}{7}}
\newlabel{eq:rho_hat}{{54}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   High dimensional Gaussian mixture according to \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 56\hbox {}\unskip \@@italiccorr )}}. We sample $200$ points total on each trial. The dashed line is Bayes accuracy $\approx 0.86$. We use the metric $\rho _1$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 52\hbox {}\unskip \@@italiccorr )}}, which is standard in energy statistics. }}{7}}
\newlabel{fig:gauss1}{{1}{7}}
\newlabel{eq:accuracy}{{55}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Synthetic Experiments}{7}}
\newlabel{eq:gauss1}{{56}{7}}
\newlabel{eq:gauss2}{{57}{7}}
\citation{Dua2017,Guvenir1998}
\citation{Dua2017,Guvenir1998}
\citation{RizzoClustering}
\citation{RizzoClustering}
\citation{Kgroups}
\citation{RizzoClustering}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   High dimensional Gaussian mixture according to \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 57\hbox {}\unskip \@@italiccorr )}}. We sample $200$ points total on each trial. The dashed line is Bayes accuracy $\approx 0.95$. We use $\rho _1$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 52\hbox {}\unskip \@@italiccorr )}} which standard metric in energy statistics. }}{8}}
\newlabel{fig:gauss2}{{2}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   Gaussian mixture with parameters \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 58\hbox {}\unskip \@@italiccorr )}}. We increase the number of sampled points in each trial. We use different metrics; see \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 52\hbox {}\unskip \@@italiccorr )}}--\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 54\hbox {}\unskip \@@italiccorr )}}. Here, kernel k-groups is more accurate than spectral clustering. }}{8}}
\newlabel{fig:gauss_n}{{3}{8}}
\newlabel{eq:20gauss}{{58}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   Same experiment as in Fig.\nobreakspace  {}3\hbox {} but with a lognormal mixture with parameters \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 58\hbox {}\unskip \@@italiccorr )}}. Again, kernel k-groups is more accurate than alternatives. The figure suggests that neither of these methods are consistent on this example since Bayes accuracy is $\approx 0.90$. }}{8}}
\newlabel{fig:loggauss_n}{{4}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   Comparison between clustering methods on unbalanced clusters. The data is normally distributed as \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 59\hbox {}\unskip \@@italiccorr )}} where we vary $m \in [0, 240]$. We use the standard metric $\rho _1$ (see \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 52\hbox {}\unskip \@@italiccorr )}}) from energy statistics. }}{8}}
\newlabel{fig:gauss_unbal}{{5}{8}}
\newlabel{eq:gauss3}{{59}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Real Data Experiment}{8}}
\citation{Dua2017,Guvenir1998}
\citation{RizzoClustering}
\citation{Dua2017,Guvenir1998}
\citation{RizzoClustering}
\citation{Kgroups}
\bibstyle{unsrt}
\bibdata{biblio.bib}
\bibcite{Szkely2013}{1}
\bibcite{Szkely2017}{2}
\bibcite{RizzoVariance}{3}
\bibcite{RizzoClustering}{4}
\bibcite{Kgroups}{5}
\bibcite{Lyons}{6}
\bibcite{Sejdinovic2013}{7}
\bibcite{Lloyd}{8}
\bibcite{MacQueen}{9}
\bibcite{Forgy}{10}
\bibcite{Smola}{11}
\bibcite{Girolami}{12}
\bibcite{Mercer}{13}
\bibcite{Dhillon2}{14}
\bibcite{Dhillon}{15}
\bibcite{Filippone}{16}
\bibcite{Hartigan1975}{17}
\bibcite{Hartigan1979}{18}
\bibcite{Telgarsky}{19}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces   Clustering the dermatology dataset of \cite  {Dua2017,Guvenir1998} with kernel k-groups using the metric $\rho _{1/2}$ (see \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 52\hbox {}\unskip \@@italiccorr )}}) from energy statistics. The table below should be compared with Table\nobreakspace  {}2 of \cite  {RizzoClustering}, for which our results are slightly more accurate. See also Table\nobreakspace  {}2\hbox {} below for clustering metrics. The classes in the vertical indicates the ground truth and the classes in the horizontal correspond to the classification obtained by kernel k-groups. We show the estimated number of points for each class. }}{9}}
\newlabel{tb:dermatology}{{1}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces   For the dataset \cite  {Dua2017,Guvenir1998} (see also Table\nobreakspace  {}1\hbox {}) we show the accuracy \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 55\hbox {}\unskip \@@italiccorr )}} and the adjusted Rand index (aRand) of several methods. In \cite  {RizzoClustering} the authors obtained $\textnormal  {aRand}=0.9195$ using an energy statistics based method, while \cite  {Kgroups} obtains $\textnormal  {aRand}=0.9188$ where points with missing entries are removed. Below we complete the missing entries with the mean. If we remove the points with missing entries, kernel k-groups provides an improvement of $\textnormal  {accuracy}=0.9637$ and $\textnormal  {aRand}=0.9396$. }}{9}}
\newlabel{tb:dermatology_accuracy}{{2}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{9}}
\newlabel{sec:conclusion}{{6}{9}}
\@writefile{toc}{\contentsline {section}{References}{9}}
\bibcite{Slonin}{20}
\bibcite{Aronszajn}{21}
\bibcite{Gretton2012}{22}
\bibcite{Berg1984}{23}
\bibcite{Malik}{24}
\bibcite{NgJordan}{25}
\bibcite{Kernighan}{26}
\bibcite{Chan}{27}
\bibcite{Yu}{28}
\bibcite{scikit-learn}{29}
\bibcite{Vassilvitskii}{30}
\bibcite{Dua2017}{31}
\bibcite{Guvenir1998}{32}
\@writefile{toc}{\contentsline {section}{Biographies}{10}}
\@writefile{toc}{\contentsline {subsection}{Guilherme Fran\c ca}{10}}
\@writefile{toc}{\contentsline {subsection}{Maria Rizzo}{10}}
\@writefile{toc}{\contentsline {subsection}{Joshua Vogelstein}{10}}
